{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dcbab07-d39c-442d-962c-2caa1bd15db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leffff/PycharmProjects/af-bot/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/leffff/PycharmProjects/sber-scs/venv/lib/python3.10/site-packages\")\n",
    "import os\n",
    "from joblib import load\n",
    "from copy import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from transformers import BertForNextSentencePrediction, BertTokenizerFast, BertConfig, AutoModel, AutoTokenizer\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a987f17a-1ec7-471e-b9d4-85144f7f330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"ai-forever/ruBert-large\"\n",
    "\"ai-forever/ruBert-base\"\n",
    "\"tinkoff-ai/response-quality-classifier-base\"\n",
    "MODEL_NAME = \"ai-forever/ruElectra-medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af049af-5ab3-4fa7-bd12-d317b2f15b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebed8f90-87ae-4816-80cf-69706a0b19d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type electra to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 576,\n",
      "  \"generator_size\": \"0.25\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2304,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForNextSentencePrediction(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(64000, 576, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 576)\n",
       "      (token_type_embeddings): Embedding(2, 576)\n",
       "      (LayerNorm): LayerNorm((576,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=576, out_features=576, bias=True)\n",
       "              (key): Linear(in_features=576, out_features=576, bias=True)\n",
       "              (value): Linear(in_features=576, out_features=576, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=576, out_features=576, bias=True)\n",
       "              (LayerNorm): LayerNorm((576,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=576, out_features=2304, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2304, out_features=576, bias=True)\n",
       "            (LayerNorm): LayerNorm((576,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=576, out_features=576, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyNSPHead(\n",
       "    (seq_relationship): Linear(in_features=576, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(MODEL_NAME)\n",
    "model = BertForNextSentencePrediction(config)\n",
    "model.cls.seq_relationship = nn.Linear(576, 2)\n",
    "model.to(DEVICE)\n",
    "print(model.config)\n",
    "# model.load_state_dict(torch.load(\"checkpoints/2.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dcd1c34-457a-4961-973e-fa96a2726627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSeq2SeqLM\n",
    "# from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# peft_config = LoraConfig(\n",
    "#     task_type=TaskType.SEQ_CLS, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63deb474-8732-4395-9a26-84bea273f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ff88a2-9796-4c89-801f-fe0f4a03812e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'relevance'],\n",
       "    num_rows: 2477321\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "dataset = load_dataset(\"Den4ikAI/russian_dialogues\", split='train').shuffle(seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16104164-d412-4528-a990-a194e81d8df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'relevance', '__index_level_0__'],\n",
       "    num_rows: 2476083\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "df = df.dropna()\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "641eeee2-b15d-450d-aa7d-2dbc94ce5592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 2476083/2476083 [02:54<00:00, 14155.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(\n",
    "        sample['question'], sample['answer'], truncation=True, padding='max_length', max_length=256\n",
    "    ),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dfe8841-41ec-425d-9908-fdc7f1ae6f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question',\n",
       " 'answer',\n",
       " 'relevance',\n",
       " '__index_level_0__',\n",
       " 'input_ids',\n",
       " 'token_type_ids',\n",
       " 'attention_mask']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(encoded_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "626eb7e3-2def-40d3-a64c-471bd0f57a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relevance', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset = encoded_dataset.remove_columns([\n",
    " 'question',\n",
    " 'answer',\n",
    " '__index_level_0__',\n",
    "])\n",
    "\n",
    "list(encoded_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "678b2c32-9cb4-403e-a03a-9c43636bef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(type='torch', columns=['relevance', 'input_ids', 'token_type_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e596c68-4fbc-47ad-afa3-e02877c60e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds, test_inds = train_test_split([i for i in range(len(encoded_dataset))], test_size=0.2)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    Subset(encoded_dataset, train_inds), \n",
    "    batch_size=76,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    Subset(encoded_dataset, test_inds), \n",
    "    batch_size=76,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43dca458-e79f-49d4-9d92-159361aae506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import BinaryAUROC\n",
    "\n",
    "ROCAUC = BinaryAUROC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5e552c8-841d-45e2-97cb-b1354d120276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion):\n",
    "    model.train()\n",
    "\n",
    "    all_probas = []\n",
    "    all_labels = []\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids, attention_masks, token_type_ids, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"token_type_ids\"], batch[\"relevance\"]\n",
    "        input_ids, attention_masks, token_type_ids, labels = input_ids.to(DEVICE), attention_masks.to(DEVICE), token_type_ids.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_masks,\n",
    "            token_type_ids=token_type_ids\n",
    "        ).logits\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        probas = output.softmax(dim=-1)\n",
    "        all_probas.append(probas.detach().cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "        \n",
    "    metrics = {\n",
    "        \"ROCAUC\": ROCAUC(torch.cat(all_probas, dim=0)[:, 1], torch.cat(all_labels, dim=0)),\n",
    "        \"Loss\": total_loss / len(dataloader)\n",
    "    }\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "def val_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    all_probas = []\n",
    "    all_labels = []\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids, attention_masks, token_type_ids, labels = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"token_type_ids\"], batch[\"relevance\"]\n",
    "        input_ids, attention_masks, token_type_ids, labels = input_ids.to(DEVICE), attention_masks.to(DEVICE), token_type_ids.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_masks,\n",
    "                token_type_ids=token_type_ids\n",
    "            ).logits\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        probas = output.softmax(dim=-1)\n",
    "        all_probas.append(probas.detach().cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "        \n",
    "\n",
    "    metrics = {\n",
    "        \"ROCAUC\": ROCAUC(torch.cat(all_probas, dim=0)[:, 1], torch.cat(all_labels, dim=0)),\n",
    "        \"Loss\": total_loss / len(dataloader)\n",
    "    }\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model, \n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    criterion\n",
    "):\n",
    "    for i in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, criterion)\n",
    "        test_loss = val_epoch(model, test_dataloader, criterion)\n",
    "\n",
    "        print(train_loss, test_loss)\n",
    "\n",
    "        torch.save(model.state_dict(), f\"electra/{i}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec231209-629b-4db4-8295-aacd2b4943f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26065/26065 [1:56:39<00:00,  3.72it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6517/6517 [10:42<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ROCAUC': tensor(0.7484), 'Loss': 0.5820043456033029} {'ROCAUC': tensor(0.7690), 'Loss': 0.5696628986165735}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26065/26065 [1:57:44<00:00,  3.69it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6517/6517 [10:45<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ROCAUC': tensor(0.7757), 'Loss': 0.5572885225995955} {'ROCAUC': tensor(0.7905), 'Loss': 0.54619039610904}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26065/26065 [1:55:01<00:00,  3.78it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6517/6517 [09:34<00:00, 11.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ROCAUC': tensor(0.8260), 'Loss': 0.5038450323406849} {'ROCAUC': tensor(0.8565), 'Loss': 0.4671186056466525}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26065/26065 [2:00:14<00:00,  3.61it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6517/6517 [10:43<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ROCAUC': tensor(0.8759), 'Loss': 0.43368750158550873} {'ROCAUC': tensor(0.8766), 'Loss': 0.4372068587116107}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███████▉                                                                                                              | 1741/26065 [07:46<1:48:43,  3.73it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m*\u001b[39m epochs)\n\u001b[1;32m      9\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m get_cosine_schedule_with_warmup(optimizer, \n\u001b[1;32m     10\u001b[0m                                     num_warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;66;03m# Default value in run_glue.py\u001b[39;00m\n\u001b[1;32m     11\u001b[0m                                     num_training_steps \u001b[38;5;241m=\u001b[39m total_steps)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 90\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, epochs, train_dataloader, test_dataloader, optimizer, scheduler, criterion)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_loop\u001b[39m(\n\u001b[1;32m     81\u001b[0m     model, \n\u001b[1;32m     82\u001b[0m     epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     criterion\n\u001b[1;32m     88\u001b[0m ):\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 90\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m         test_loss \u001b[38;5;241m=\u001b[39m val_epoch(model, test_dataloader, criterion)\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28mprint\u001b[39m(train_loss, test_loss)\n",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, scheduler, criterion)\u001b[0m\n\u001b[1;32m     26\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 29\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m probas \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/PycharmProjects/af-bot/venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/af-bot/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/af-bot/venv/lib/python3.10/site-packages/madgrad/madgrad.py:201\u001b[0m, in \u001b[0;36mMADGRAD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    198\u001b[0m     z \u001b[38;5;241m=\u001b[39m x0\u001b[38;5;241m.\u001b[39maddcdiv(s, rms, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# p is a moving average of z\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     \u001b[43mp_data_fp32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mck\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(z, alpha\u001b[38;5;241m=\u001b[39mck)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m decouple_decay:\n\u001b[1;32m    204\u001b[0m     p_data_fp32\u001b[38;5;241m.\u001b[39madd_(p_old, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mlr\u001b[38;5;241m*\u001b[39mdecay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from madgrad import MADGRAD\n",
    "\n",
    "epochs = 20\n",
    "optimizer = MADGRAD([\n",
    "        {\"params\": model.cls.parameters(), \"lr\": 1e-4},\n",
    "        {\"params\": model.bert.parameters(), \"lr\": 5e-6},\n",
    "])\n",
    "total_steps = int(len(train_dataloader) * epochs)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                    num_training_steps = total_steps)\n",
    "\n",
    "train_loop(\n",
    "    model=model, \n",
    "    epochs=epochs,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=val_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
